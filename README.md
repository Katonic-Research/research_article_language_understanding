# research_article_language_understanding

This app is about the research articles for **Improving Language Understanding by Generative Pre-Training**

## Description

This article shows that by generatively pre-training a language model on a wide range of unlabeled text and then discriminatively fine-tuning on each individual test, significant improvements on the tasks may be achieved. Unlike prior methods, here applied task-aware input transformations during fine-tuning to accomplish effective transfer with a minimum of model architecture changes. This article mentions the use of a variety of criteria for natural language understanding to show the efficacy of the method.In 9 out of the 12 tasks investigated, the general task-agnostic model significantly outperforms state-of-the-art discriminatively trained models that use architectures tailored for each task. Examples include absolute gains of 8.9% in commonsense reasoning (Stories Cloze Test), 5.7% in question-answering (RACE), and 1.5% in textual entailment (MultiNLI).
